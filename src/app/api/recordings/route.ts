import { NextRequest, NextResponse } from "next/server";
import { auth } from "@/lib/auth";
import { put } from "@vercel/blob";
import prisma from "@/lib/prisma";
import openai from "@/lib/openai";
import { generateShareToken } from "@/lib/utils";

export async function POST(req: NextRequest) {
  try {
    const session = await auth();
    if (!session?.user?.id) {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    }

    const formData = await req.formData();
    const audioFile = formData.get("audio") as File;
    const title = (formData.get("title") as string) || "Untitled Recording";

    if (!audioFile) {
      return NextResponse.json(
        { error: "No audio file provided" },
        { status: 400 }
      );
    }

    // Upload audio to Vercel Blob
    const blob = await put(`recordings/${session.user.id}/${Date.now()}.webm`, audioFile, {
      access: "public",
      contentType: audioFile.type || "audio/webm",
    });

    // Create recording entry in database
    const recording = await prisma.recording.create({
      data: {
        title,
        audioUrl: blob.url,
        status: "processing",
        shareToken: generateShareToken(),
        userId: session.user.id,
      },
    });

    // Process in background - transcribe and summarize
    processRecording(recording.id, blob.url).catch(console.error);

    return NextResponse.json({ recording }, { status: 201 });
  } catch (error) {
    console.error("Upload error:", error);
    return NextResponse.json(
      { error: "Failed to upload recording" },
      { status: 500 }
    );
  }
}

async function processRecording(recordingId: string, audioUrl: string) {
  try {
    // Update status to transcribing
    await prisma.recording.update({
      where: { id: recordingId },
      data: { status: "transcribing" },
    });

    // Fetch audio file for transcription
    const audioResponse = await fetch(audioUrl);
    const audioBuffer = await audioResponse.arrayBuffer();
    const audioFile = new File([audioBuffer], "audio.webm", {
      type: "audio/webm",
    });

    // Transcribe using OpenAI Whisper
    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: "whisper-1",
      response_format: "text",
    });

    // Update status to summarizing
    await prisma.recording.update({
      where: { id: recordingId },
      data: {
        transcript: transcription,
        status: "summarizing",
      },
    });

    // Generate summary using GPT
    const summaryResponse = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        {
          role: "system",
          content: `You are a professional meeting intelligence assistant similar to Otter.ai. Analyze the transcript and produce a polished, structured summary using this EXACT markdown format:

## Overview
Write a concise 2-3 sentence executive overview of what was discussed.

## Key Topics Discussed
For each major topic, create a section with details:
- **Topic Name**: Brief explanation of what was discussed about this topic
- **Topic Name**: Brief explanation

## Action Items
List any tasks, to-dos, or follow-ups mentioned (if none, write "No specific action items identified"):
- [ ] Action item with responsible person (if mentioned)
- [ ] Another action item

## Key Decisions
List any decisions that were made during the discussion (if none, write "No specific decisions recorded"):
- Decision description
- Another decision

## Important Details & Notes
- Any important facts, numbers, dates, or details mentioned
- Names of people or organizations referenced
- Any deadlines or timelines mentioned

---
*Summary generated by VoiceScribe AI*

RULES:
- Use clean, professional language
- Be concise but comprehensive
- Use proper markdown formatting with headers, bold, bullet points, and checkboxes
- If the transcript is short or informal, still maintain the professional structure but adapt content accordingly
- Never fabricate information not present in the transcript`,
        },
        {
          role: "user",
          content: `Analyze and summarize the following meeting/audio transcript:\n\n${transcription}`,
        },
      ],
      max_tokens: 2000,
    });

    const summary = summaryResponse.choices[0]?.message?.content || "No summary generated.";

    // Update recording with results
    await prisma.recording.update({
      where: { id: recordingId },
      data: {
        summary,
        status: "completed",
      },
    });
  } catch (error) {
    console.error("Processing error:", error);
    await prisma.recording.update({
      where: { id: recordingId },
      data: { status: "error" },
    });
  }
}
